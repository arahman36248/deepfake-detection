"""
Model training script optimized for i5 11th Gen CPU
For Parrot Security OS
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
import json
from pathlib import Path
from tqdm import tqdm
import numpy as np
from deepfake_model.model_utils import create_model

class DeepfakeDataset(Dataset):
    """
    Dataset for deepfake images
    
    Expected structure:
    dataset/
        real/
            image1.jpg
            image2.jpg
            ...
        fake/
            image1.jpg
            image2.jpg
            ...
    """
    
    def __init__(self, root_dir, transform=None):
        self.root_dir = Path(root_dir)
        self.transform = transform
        self.images = []
        self.labels = []
        
        # Load real images (label: 0)
        real_dir = self.root_dir / 'real'
        if real_dir.exists():
            for img_path in real_dir.glob('*.jpg') or real_dir.glob('*.png'):
                self.images.append(str(img_path))
                self.labels.append(0)
        
        # Load fake images (label: 1)
        fake_dir = self.root_dir / 'fake'
        if fake_dir.exists():
            for img_path in fake_dir.glob('*.jpg') or fake_dir.glob('*.png'):
                self.images.append(str(img_path))
                self.labels.append(1)
        
        print(f"Loaded {len(self.images)} images ({len([l for l in self.labels if l == 0])} real, {len([l for l in self.labels if l == 1])} fake)")
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        img_path = self.images[idx]
        label = self.labels[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
            
            if self.transform:
                image = self.transform(image)
            
            return image, label
        except Exception as e:
            print(f"Error loading {img_path}: {e}")
            # Return a black image on error
            return torch.zeros(3, 224, 224), label


def train_model(
    dataset_dir='datasets/deepfake',
    model_type='simple',
    epochs=10,
    batch_size=8,  # Small batch for i5 CPU
    learning_rate=0.001,
    save_path='models/trained_model.pth'
):
    """
    Train deepfake detection model on CPU
    
    Args:
        dataset_dir: Path to dataset directory
        model_type: 'simple' (ResNet18) or 'cnn_lstm'
        epochs: Number of training epochs
        batch_size: Batch size (keep small for CPU)
        learning_rate: Learning rate
        save_path: Where to save trained model
    """
    
    print("=" * 70)
    print("DEEPFAKE DETECTION MODEL TRAINING")
    print("=" * 70)
    print(f"Device: CPU (Intel i5 11th Gen)")
    print(f"Model: {model_type}")
    print(f"Epochs: {epochs}")
    print(f"Batch Size: {batch_size}")
    print(f"Learning Rate: {learning_rate}")
    print("=" * 70)
    
    # Setup device
    device = torch.device('cpu')
    print(f"Using device: {device}")
    
    # Data transforms
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # Load dataset
    print("\nLoading dataset...")
    dataset = DeepfakeDataset(dataset_dir, transform=transform)
    
    if len(dataset) == 0:
        print("ERROR: No images found in dataset!")
        print(f"Expected structure:")
        print(f"  {dataset_dir}/")
        print(f"    real/  (real images)")
        print(f"    fake/  (fake images)")
        return
    
    # Split dataset (80% train, 20% validation)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )
    
    print(f"Training set: {len(train_dataset)} images")
    print(f"Validation set: {len(val_dataset)} images")
    
    # Data loaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=2,  # Use 2 workers on i5
        pin_memory=False
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size,
        num_workers=2,
        pin_memory=False
    )
    
    # Create model
    print("\nCreating model...")
    model = create_model(model_type, device)
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # Loss and optimizer
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=2, verbose=True
    )
    
    # Training history
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': []
    }
    
    best_val_acc = 0.0
    
    # Training loop
    print("\nStarting training...")
    for epoch in range(epochs):
        print(f"\n{'='*70}")
        print(f"Epoch {epoch+1}/{epochs}")
        print(f"{'='*70}")
        
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        with tqdm(train_loader, desc='Training', unit='batch') as pbar:
            for images, labels in pbar:
                images = images.to(device)
                labels = labels.to(device).float()
                
                # Forward pass
                optimizer.zero_grad()
                outputs = model(images).squeeze()
                loss = criterion(outputs, labels)
                
                # Backward pass
                loss.backward()
                optimizer.step()
                
                # Statistics
                train_loss += loss.item()
                predicted = (outputs > 0.5).float()
                train_correct += (predicted == labels).sum().item()
                train_total += labels.size(0)
                
                # Update progress bar
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{100 * train_correct / train_total:.2f}%'
                })
        
        # Calculate training metrics
        avg_train_loss = train_loss / len(train_loader)
        train_acc = 100 * train_correct / train_total
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            with tqdm(val_loader, desc='Validation', unit='batch') as pbar:
                for images, labels in pbar:
                    images = images.to(device)
                    labels = labels.to(device).float()
                    
                    outputs = model(images).squeeze()
                    loss = criterion(outputs, labels)
                    
                    val_loss += loss.item()
                    predicted = (outputs > 0.5).float()
                    val_correct += (predicted == labels).sum().item()
                    val_total += labels.size(0)
                    
                    pbar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'acc': f'{100 * val_correct / val_total:.2f}%'
                    })
        
        # Calculate validation metrics
        avg_val_loss = val_loss / len(val_loader)
        val_acc = 100 * val_correct / val_total
        
        # Update history
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(val_acc)
        
        # Print epoch summary
        print(f"\nEpoch {epoch+1} Summary:")
        print(f"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%")
        print(f"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%")
        
        # Learning rate scheduling
        scheduler.step(avg_val_loss)
        
        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            torch.save(model.state_dict(), save_path)
            print(f"  âœ“ New best model saved! (Val Acc: {val_acc:.2f}%)")
    
    # Final summary
    print("\n" + "="*70)
    print("TRAINING COMPLETE")
    print("="*70)
    print(f"Best Validation Accuracy: {best_val_acc:.2f}%")
    print(f"Model saved to: {save_path}")
    
    # Save training history
    history_path = save_path.replace('.pth', '_history.json')
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)
    print(f"Training history saved to: {history_path}")
    
    return model, history


if __name__ == '__main__':
    # Example usage
    import argparse
    
    parser = argparse.ArgumentParser(description='Train deepfake detection model')
    parser.add_argument('--dataset', type=str, default='datasets/deepfake',
                       help='Path to dataset directory')
    parser.add_argument('--model', type=str, default='simple',
                       choices=['simple', 'cnn_lstm'],
                       help='Model type')
    parser.add_argument('--epochs', type=int, default=10,
                       help='Number of epochs')
    parser.add_argument('--batch-size', type=int, default=8,
                       help='Batch size (keep small for CPU)')
    parser.add_argument('--lr', type=float, default=0.001,
                       help='Learning rate')
    parser.add_argument('--output', type=str, default='models/trained_model.pth',
                       help='Output model path')
    
    args = parser.parse_args()
    
    train_model(
        dataset_dir=args.dataset,
        model_type=args.model,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        save_path=args.output
    )
